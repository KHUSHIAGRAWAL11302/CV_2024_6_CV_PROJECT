{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDlyeLPQVZvI",
        "outputId": "807990b3-7ea5-4cf7-fd3a-0b1d1161080c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "base_path = \"/content/drive/MyDrive/augmented_dataset\"\n",
        "\n",
        "train_images = os.path.join(base_path, \"images/train\")\n",
        "train_masks  = os.path.join(base_path, \"masks/train\")\n",
        "\n",
        "val_images   = os.path.join(base_path, \"images/valid\")\n",
        "val_masks    = os.path.join(base_path, \"masks/valid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def get_unique_classes(mask_dir):\n",
        "    unique_classes = set()\n",
        "    for mask_file in os.listdir(mask_dir):\n",
        "        mask_path = os.path.join(mask_dir, mask_file)\n",
        "        # Check if the item is a file before opening it\n",
        "        if os.path.isfile(mask_path):\n",
        "            mask = np.array(Image.open(mask_path))\n",
        "            unique_classes.update(np.unique(mask))\n",
        "    return sorted(list(unique_classes))\n",
        "\n",
        "# Detect classes from training masks\n",
        "classes = get_unique_classes(train_masks)\n",
        "print(\"Detected Classes:\", classes)\n",
        "\n",
        "# Optional: Remove ignore class (255)\n",
        "if 255 in classes:\n",
        "    classes.remove(255)\n",
        "\n",
        "NUM_CLASSES = len(classes)\n",
        "print(\"Total Classes:\", NUM_CLASSES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnmv2FunUMwa",
        "outputId": "a6f7f9b5-21a2-40f8-be6d-b4d4dffe8a5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Classes: [np.uint8(0), np.uint8(1), np.uint8(2), np.uint8(3), np.uint8(4), np.uint8(5), np.uint8(6), np.uint8(7), np.uint8(8), np.uint8(9), np.uint8(10), np.uint8(11), np.uint8(12), np.uint8(13), np.uint8(14), np.uint8(15), np.uint8(16), np.uint8(17), np.uint8(18), np.uint8(19), np.uint8(20), np.uint8(21), np.uint8(22), np.uint8(23), np.uint8(24), np.uint8(25), np.uint8(26), np.uint8(27), np.uint8(28), np.uint8(29), np.uint8(30), np.uint8(31), np.uint8(32), np.uint8(33), np.uint8(34), np.uint8(35), np.uint8(36), np.uint8(37), np.uint8(38), np.uint8(39), np.uint8(40), np.uint8(41), np.uint8(42), np.uint8(43), np.uint8(44), np.uint8(45), np.uint8(46), np.uint8(47), np.uint8(48), np.uint8(49), np.uint8(50), np.uint8(51), np.uint8(52), np.uint8(53), np.uint8(54), np.uint8(55), np.uint8(56), np.uint8(57), np.uint8(58), np.uint8(59), np.uint8(60), np.uint8(61), np.uint8(62), np.uint8(63), np.uint8(64), np.uint8(65), np.uint8(66), np.uint8(67), np.uint8(68), np.uint8(69), np.uint8(70), np.uint8(71), np.uint8(72), np.uint8(73), np.uint8(74), np.uint8(75), np.uint8(76), np.uint8(77), np.uint8(78), np.uint8(79), np.uint8(80), np.uint8(81), np.uint8(82), np.uint8(83), np.uint8(84), np.uint8(85), np.uint8(86), np.uint8(87), np.uint8(88), np.uint8(89), np.uint8(90), np.uint8(91), np.uint8(92), np.uint8(93), np.uint8(94), np.uint8(95), np.uint8(96), np.uint8(97), np.uint8(98), np.uint8(99), np.uint8(100), np.uint8(101), np.uint8(102), np.uint8(103), np.uint8(104), np.uint8(105), np.uint8(106), np.uint8(107), np.uint8(108), np.uint8(109), np.uint8(110), np.uint8(111), np.uint8(112), np.uint8(113), np.uint8(114), np.uint8(115), np.uint8(116), np.uint8(117), np.uint8(118), np.uint8(119), np.uint8(120), np.uint8(121), np.uint8(122), np.uint8(123), np.uint8(124), np.uint8(125), np.uint8(126), np.uint8(127), np.uint8(128), np.uint8(129), np.uint8(130), np.uint8(131), np.uint8(132), np.uint8(133), np.uint8(134), np.uint8(135), np.uint8(136), np.uint8(137), np.uint8(138), np.uint8(139), np.uint8(140), np.uint8(141), np.uint8(142), np.uint8(143), np.uint8(144), np.uint8(145), np.uint8(146), np.uint8(147), np.uint8(148), np.uint8(149), np.uint8(150), np.uint8(151), np.uint8(152), np.uint8(153), np.uint8(154), np.uint8(155), np.uint8(156), np.uint8(157), np.uint8(158), np.uint8(159), np.uint8(160), np.uint8(161), np.uint8(162), np.uint8(163), np.uint8(164), np.uint8(165), np.uint8(166), np.uint8(167), np.uint8(168), np.uint8(169), np.uint8(170), np.uint8(171), np.uint8(172), np.uint8(173), np.uint8(174), np.uint8(175), np.uint8(176), np.uint8(177), np.uint8(178), np.uint8(179), np.uint8(180), np.uint8(181), np.uint8(182), np.uint8(183), np.uint8(184), np.uint8(185), np.uint8(186), np.uint8(187), np.uint8(188), np.uint8(189), np.uint8(190), np.uint8(191), np.uint8(192), np.uint8(193), np.uint8(194), np.uint8(195), np.uint8(196), np.uint8(197), np.uint8(198), np.uint8(199), np.uint8(200), np.uint8(201), np.uint8(202), np.uint8(203), np.uint8(204), np.uint8(205), np.uint8(206), np.uint8(207), np.uint8(208), np.uint8(209), np.uint8(210), np.uint8(211), np.uint8(212), np.uint8(213), np.uint8(214), np.uint8(215), np.uint8(216), np.uint8(217), np.uint8(218), np.uint8(219), np.uint8(220), np.uint8(221), np.uint8(222), np.uint8(223), np.uint8(224), np.uint8(225), np.uint8(226), np.uint8(227), np.uint8(228), np.uint8(229), np.uint8(230), np.uint8(231), np.uint8(232), np.uint8(233), np.uint8(234), np.uint8(235), np.uint8(236), np.uint8(237), np.uint8(238), np.uint8(239), np.uint8(240), np.uint8(241), np.uint8(242), np.uint8(243), np.uint8(244), np.uint8(245), np.uint8(246), np.uint8(247), np.uint8(248), np.uint8(249), np.uint8(250), np.uint8(251), np.uint8(252), np.uint8(253), np.uint8(254), np.uint8(255)]\n",
            "Total Classes: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, feature_extractor, image_size=(512, 512)): # Added image_size\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        # Filter out directories in the mask directory\n",
        "        self.masks = sorted([f for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.image_size = image_size # Store image_size\n",
        "        # Define a transform to resize images and masks\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self.image_size), # Resize to the specified size\n",
        "            transforms.ToTensor(), # Convert to tensor\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # Apply transforms to image and mask\n",
        "        image = self.transform(image)  # Resize and convert image to tensor\n",
        "        mask = self.transform(mask)  # Resize and convert mask to tensor\n",
        "        mask = torch.round(mask).long()  # Round mask to integers after scaling\n",
        "\n",
        "        # Apply feature extractor to image\n",
        "        encoded = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = encoded[\"pixel_values\"].squeeze()\n",
        "\n",
        "        # Convert mask to tensor [H, W]\n",
        "        # No longer need this, as it was handled by the transform\n",
        "        # mask_tensor = torch.tensor(np.array(mask), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": mask.squeeze() # Use the resized mask tensor\n",
        "        }"
      ],
      "metadata": {
        "id": "zLZ44svk1jNv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=False)\n",
        "\n",
        "train_dataset = SegmentationDataset(train_images, train_masks, feature_extractor)\n",
        "val_dataset = SegmentationDataset(val_images, val_masks, feature_extractor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxGalTXp1xzp",
        "outputId": "01b362b8-9e2e-444b-f28d-926299628f2c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "    num_labels=NUM_CLASSES,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEjC1nUk2QqA",
        "outputId": "ba82be39-0478-438e-81b6-68086d2555e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([255]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([255, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNFaiS7A2fSl",
        "outputId": "aba3c8ea-4b8e-4e39-92ea-52f8ace7b0a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 375/375 [04:00<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training Loss: 3.4230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 375/375 [03:47<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Training Loss: 0.9354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 375/375 [03:45<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Training Loss: 0.4441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 375/375 [03:46<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Training Loss: 0.3475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 375/375 [03:47<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Training Loss: 0.3204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, num_classes):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    ious = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_pixels += torch.numel(preds)\n",
        "\n",
        "            for cls in range(num_classes):\n",
        "                pred_inds = preds == cls\n",
        "                label_inds = labels == cls\n",
        "                intersection = (pred_inds & label_inds).sum().item()\n",
        "                union = (pred_inds | label_inds).sum().item()\n",
        "                if union != 0:\n",
        "                    ious.append(intersection / union)\n",
        "\n",
        "    pixel_acc = total_correct / total_pixels\n",
        "    mean_iou = sum(ious) / len(ious)\n",
        "\n",
        "    print(f\"Pixel Accuracy: {pixel_acc:.4f}\")\n",
        "    print(f\"Mean IoU: {mean_iou:.4f}\")\n"
      ],
      "metadata": {
        "id": "bHkMOnK-2j5p"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tL6YrB4OeM-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}